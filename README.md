# ola-assignment-1-open-ended-learning-approaches-for-3d-object-recognition-solved
**TO GET THIS SOLUTION VISIT:** [OLA Assignment 1-Open-Ended Learning Approaches for 3D Object Recognition Solved](https://www.ankitcodinghub.com/product/ola-assignment-1-open-ended-learning-approaches-for-3d-object-recognition-solved/)


---

üì© **If you need this solution or have special requests:** **Email:** ankitcoding@gmail.com  
üì± **WhatsApp:** +1 419 877 7882  
üìÑ **Get a quote instantly using this form:** [Ask Homework Questions](https://www.ankitcodinghub.com/services/ask-homework-questions/)

*We deliver fast, professional, and affordable academic help.*

---

<h2>Description</h2>



<div class="kk-star-ratings kksr-auto kksr-align-center kksr-valign-top" data-payload="{&quot;align&quot;:&quot;center&quot;,&quot;id&quot;:&quot;91364&quot;,&quot;slug&quot;:&quot;default&quot;,&quot;valign&quot;:&quot;top&quot;,&quot;ignore&quot;:&quot;&quot;,&quot;reference&quot;:&quot;auto&quot;,&quot;class&quot;:&quot;&quot;,&quot;count&quot;:&quot;1&quot;,&quot;legendonly&quot;:&quot;&quot;,&quot;readonly&quot;:&quot;&quot;,&quot;score&quot;:&quot;5&quot;,&quot;starsonly&quot;:&quot;&quot;,&quot;best&quot;:&quot;5&quot;,&quot;gap&quot;:&quot;4&quot;,&quot;greet&quot;:&quot;Rate this product&quot;,&quot;legend&quot;:&quot;5\/5 - (1 vote)&quot;,&quot;size&quot;:&quot;24&quot;,&quot;title&quot;:&quot;OLA Assignment 1-Open-Ended Learning Approaches for 3D Object Recognition Solved&quot;,&quot;width&quot;:&quot;138&quot;,&quot;_legend&quot;:&quot;{score}\/{best} - ({count} {votes})&quot;,&quot;font_factor&quot;:&quot;1.25&quot;}">

<div class="kksr-stars">

<div class="kksr-stars-inactive">
            <div class="kksr-star" data-star="1" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="2" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="3" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="4" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" data-star="5" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>

<div class="kksr-stars-active" style="width: 138px;">
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
            <div class="kksr-star" style="padding-right: 4px">


<div class="kksr-icon" style="width: 24px; height: 24px;"></div>
        </div>
    </div>
</div>


<div class="kksr-legend" style="font-size: 19.2px;">
            5/5 - (1 vote)    </div>
    </div>
<div class="page" title="Page 1">
<div class="layoutArea">
<div class="column">
Open-Ended Learning Approaches for 3D Object Recognition

Assignment overview

Cognitive science revealed that humans learn to recognize object categories ceaselessly over time. This ability allows them to adapt to new environments, by enhancing their knowledge from the accumulation of experiences and the conceptualization of new object categories. Taking this theory as an inspiration, we seek to create an interactive object recognition system that can learn 3D object categories in an open-ended fashion. In this project, ‚Äúopen-ended‚Äù implies that the set of categories to be learned is not known in advance. The training instances are extracted from on-line experiences of a robot, and thus become gradually available over time, rather than being completely available at the beginning of the learning process.

In this assignment, students have to optimize an open-ended learning approach for 3D object recognition and get familiar with the basic functionalities of ROS. We break this assignment down into two parts:

1. The first part is about implementing/optimizing offline 3D object recognition systems, which take an object view as input and produces the category label as output (e.g., apple, mug, fork, etc).

2. Thesecondpartofthisassignmentisdedicatedto

testing object recognition approaches in an open-

ended fashion. In this assignment, the number of

categories is not pre-defined in advance and the

knowledge of agent/robot is increasing over time by interacting with a simulated teacher using three actions: teach, ask,andcorrect(seeFig.1).

Further details of these assignments are explained in the following sections. To make your life easier, we provide a virtual machine that has all the necessary programs, codes, dataset, libraries, and packages. We also offer template codes for each assignment.

If you are not familiar with the concept of ROS, please follow the beginner level of ROS Tutorials. For all student, going over all basic beginner level tutorials is strongly recommended.

 I recommend installing MATLAB on your machine since the output of experiments are automatically visualized in MATLAB. You can download it from download portal or use an online version provided by the university.

 As an alternative, we also provide a python script to visualize the generated MATLAB plots automatically. Youcanuseitasfollows:$ python3 matlab_plots_parser.py -htochecktheinstruction

Policies

‚Ä¢ Feel free to collaborate on solving the problem but please write your code/report individually. In particular, do

not copy code/text from other students or online resources.

‚Ä¢ You are not allowed to publish any part of this code online or claim that you have written it. It does not matter, even if the code is partially used. If you want to publish your results as a scientific paper or use this framework in other projects, contact Hamidreza Kasaei (hamidreza.kasaei@rug.nl) directly and discuss the case explicitly.

</div>
</div>
<div class="layoutArea">
<div class="column">
Predicted category

</div>
<div class="column">
Teach Ask

Correct

</div>
<div class="column">
simulate teacher

dataset of objects

</div>
</div>
<div class="layoutArea">
<div class="column">
Figure 1: Abstract architecture for interaction between the simu- lated teacher and the learning agent.

</div>
</div>
</div>
<div class="page" title="Page 2">
<div class="layoutArea">
<div class="column">
Part I: Offline 3D object recognition setting (50%)

In this assignment, we assume that an object has already been segmented from the scene and we want to recognize its label. We intent to use an instance-based learning (IBL) approach to form new categories. From a general perspective, IBL approaches can be viewed as a combination of an object representation approach, a similarity measure, and a classification rule. Therefore, we represent an object category by storing the representation of objects‚Äô views of the category. Furthermore, the choice of the object representation and similarity measure have impacts on the recognition performance as shown in Fig. 2.

</div>
</div>
<div class="section">
<div class="layoutArea">
<div class="column">
offline

Datasets of objects

point cloud

</div>
<div class="column">
object‚Äôs point cloud

</div>
<div class="column">
Object Representation

</div>
<div class="column">
feature vector

</div>
<div class="column">
Object Category Models

</div>
<div class="column">
object category models

Object Recognition

</div>
</div>
<div class="layoutArea">
<div class="column">
real-time

Scene

</div>
<div class="column">
Object Segmentation

</div>
<div class="column">
object‚Äôs point cloud

</div>
<div class="column">
Object Representation

</div>
<div class="column">
feature vector

</div>
<div class="column">
recognition result

</div>
</div>
</div>
<div class="layoutArea">
<div class="column">
Figure 2: The components used in a 3D object recognition system.

In the case of the similarity measure, since the object representation module represents an object as a normalized histogram, the dissimilarity between two histograms can be computed by different distance functions. In this work, you need to select five out of 14 distance functions that are dissimilar from each other. This policy will increase the chance that different functions lead to different results. The following 14 functions have been implemented and exist in the RACE framework:

Euclidean, Manhattan, œá2, Pearson, Neyman, Canberra, KL divergence, symmetric KL divergence, Motyka, Cosine, Dice, Bhattacharyya, Gower, and Sorensen.

 For the mathematical equations of these functions, we refer the reader to a comprehensive survey on distance/similarity measures provided by S. Cha (1).

The main intuition behind using instance-based learning in this study is that, IBL serves as a baseline approach for evaluating the object representations used in object recognition. More advance approaches, e.g., SVM-based and Bayesian learning, can be easily adapted.

To examine the performance of an object recognition, we provide a K-fold cross-validation procedure. K-fold cross- validation is one of the most widely used methods for estimating the generalization performance of a learning algorithm. In this evaluation protocol, K folds are randomly created by dividing the dataset into K equal-sized subsets, where each subset contains examples from all the categories. In each iteration, a single fold is used for testing, and the remaining nine folds are used as training data. For K-fold cross-validation, we set K to 10, as is generally recommended in the literature. This type of evaluation is useful not only for parameter tuning but also for comparing the performance of your method with other approaches described in the literature.

√ê What we offer for this part

‚Ä¢ A detail instruction about how to run each of the experiments

‚Ä¢ A ROS-based cpp code for 10 fold-cross validation: we have implemented a set of object representation approaches and different distance functions for object recognition purpose. You need to study each approach in depth and optimize its parameters.

‚Ä¢ A ROS-based cpp code for K-fold-cross validation with various deep learning architectures as object representa- tion and a set of distance functions for object recognition purpose. You need to study each approach in depth and optimize its parameters.

</div>
</div>
<div class="layoutArea">
<div class="column">
2

</div>
</div>
</div>
<div class="page" title="Page 3">
<div class="layoutArea">
<div class="column">
<ul>
<li>Sample bash scripts for running a bunch of experiments based on GOOD descriptor (hand-crafted), and MobileNetV2 architecture (deep transfer learning), find them out in rug_kfold_cross_validation/result).</li>
<li>A python script to visualize the confusion matrix as the output. Run python3 matlab_plots_parser.py -p PATH_TO_EXP_DIR/ ‚Äìoffline to visualize the confusion matrix. You can use [-h] to see the instruction.
√ê Your tasks for this part

For this assignment, students will work partly individual and partly in groups of two. Each student needs

to optimize one hand-crafted and one deep learning based 3D object recognition algorithm. Therefore, each group will

have four set of results. The students will need to write up the report together by discussing the selected approaches and

comparing the obtained results in terms of instance accuracy (accmicro = # true predictions ), average class accuracy # predictions

(accmacro = K1 Ùè∞ÄKi=1 acci), and computation time. Note that you need to report average class accuracy to address class imbalance, since instance accuracy is sensitive to class imbalance.

You can think about the following groups:

‚Ä¢ (a) Hand-crafted object representation + IBL approach + K-NN:

‚Äì list of available descriptors: [GOOD, ESF, VFH, GRSD] ‚Äì distance functions as mentioned above

‚Äì K‚àà[1,3,5,7,9]

‚Ä¢ (b) Deep transfer learning based object representation + IBL + K-NN

<ul>
<li>‚Äì &nbsp;list of available network architectures: [mobileNet, mobileNetV2, vgg16_fc1, vgg16_fc2, vgg19_fc1, vgg19_fc2, xception, resnet50, denseNet121, denseNet169, densenet201, nasnetLarge, nasnetMobile, inception, inceptionResnet]</li>
<li>‚Äì &nbsp;list of available element-wise pooling: [AVG, MAX, APP (append)]</li>
<li>‚Äì &nbsp;distance functions as mentioned above,</li>
<li>‚Äì &nbsp;K‚àà[1,3,5,7,9]
In this assignment, we use a small-scaled RGB-D dataset to evaluate the performance of diffident configurations of each approach. In particular, we use Restaurant RGB-D Object Dataset, which has a small number of classes with significant intra-class variation. Therefore, it is a suitable dataset for performing extensive sets of experiments to tuning the parameters of each approach.

√ê How to run the experiments

We created a launch file for each of the mentioned object recognition Algorithms. A Launch file provides a convenient

way to start up the roscore, and multiple nodes and set the parameters‚Äô value (read more about launch file here). Before running an experiment, check the following:

‚Ä¢ You have to update the value of different parameters of the system in the launch file (e.g., rug_kfold_cross_validation/launch/kfold_cross_validation.launch)

 You can also set the value of a parameter when you launch an experiment using the following command: $ roslaunch package_name launch_file.launch parameter:=value This option is useful for running a bunch of experiments using a bash/python script

 The system configuration is reported at the beginning of the report file of the experiment. Therefore, you can use it as a way to debug/double-check the system‚Äôs parameters.
</li>
</ul>
</li>
</ul>
7 For the hand-crafted based object recognition approaches:

After adjusting all necessary parameters in the launch file, you can run an experiment using the following command:

<pre>$ roslaunch rug_kfold_cross_validation kfold_cross_validation_hand_crafted_descriptor.launch
</pre>
</div>
</div>
<div class="layoutArea">
<div class="column">
3

</div>
</div>
</div>
<div class="page" title="Page 4">
<div class="layoutArea">
<div class="column">
7 For the deep transfer learning based object representation approaches:

After adjusting all necessary parameters in the launch file, you need to open three terminals and use the following

commands to run a deep transfer learning based object recognition experiment:

aÃÄ MobileNetV2 Architecture

$ roscore

$ rosrun rug_deep_feature_extraction multi_view_RGBD_object_representation.py mobileNetV2 $ roslaunch rug_kfold_cross_validation kfold_cross_validation_RGBD_deep_learning_descriptor.launch ortho graphic_image_resolution:=150 base_network:=mobileNetV2 K_for_KNN:=3 name_of_approach:=TEST

aÃÄ VGG16 Architecture

$ roscore

$ rosrun rug_deep_feature_extraction multi_view_RGBD_object_representation.py vgg16_fc1

$ roslaunch rug_kfold_cross_validation kfold_cross_validation_RGBD_deep_learning_descriptor.launch ortho graphic_image_resolution:=150 base_network:=vgg16_fc1 K_for_KNN:=3 name_of_approach:=TEST

√ê What are the outputs of each experiment

‚Ä¢ Results of an experiment, including a detail summary, and a confusion matrix (see Fig. 3 and 4), will be saved in:

<pre>      $HOME/student_ws/rug_kfold_cross_validation/result/experiment_1/
</pre>
After each experiment, you need to either rename the experiment_1 folder or move it to another folder, otherwise its contents will be replaced by the results of a new experiment.

‚Ä¢ We also report a summary of a bunch of experiments in a txt file in the following path (see Fig. 5): rug_kfold_cross_validation/result/results_of_name_of_approach_experiments.txt

Figure3: Adetailedsummaryofanexperiment:thesystemconfigurationisspecifiedatthebeginningofthefile.Asummaryofthe experiment is subsequently reported. Objects that are incorrectly classified are highlighted by double dash-line, e.g., No. 9.

</div>
</div>
<div class="layoutArea">
<div class="column">
4

</div>
</div>
</div>
<div class="page" title="Page 5">
<div class="section">
<table>
<tbody>
<tr>
<td>
<div class="layoutArea">
<div class="column">
Bottle Bowl Flask Fork Knife Mug Plate Spoon Teapot Vase

</div>
</div>
<div class="layoutArea">
<div class="column">
100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 20 0 0 0 0 0 0 0 0 0

0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0 34 0 0 0 0 0 0 0 0

0.0% 0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0 0 22 0 0 0 0 0 0 0

0.0% 0.0% 0.0% 72.7% 9.1% 0.0% 0.0% 18.2% 0.0% 0.0% 0008100200

0.0% 0.0% 0.0% 0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0 0 0 0 28 0 0 0 0 0

0.0% 0.0% 0.0% 0.0% 0.0% 96.1% 0.0% 0.0% 3.9% 0.0% 0 0 0 0 0 49 0 0 2 0

0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 95.7% 0.0% 0.0% 4.3% 0 0 0 0 0 0 22 0 0 1

3.0% 0.0% 0.0% 9.1% 12.1% 0.0% 0.0% 75.8% 0.0% 0.0% 1 0 0 3 4 0 0 25 0 0

0.0% 0.0% 0.0% 0.0% 0.0% 3.6% 0.0% 0.0% 96.4% 0.0% 0 0 0 0 0 1 0 0 27 0

0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 100.0% 0 0 0 0 0 0 0 0 0 57

</div>
</div>
<div class="layoutArea">
<div class="column">
Bottle

</div>
<div class="column">
Bowl Flask

</div>
<div class="column">
Fork Knife

Target Category

</div>
<div class="column">
Spoon

</div>
<div class="column">
Teapot Vase

</div>
</div>
<div class="layoutArea">
<div class="column">
Accuracy: 95.11%

</div>
<div class="column">
100 90 80 70 60 50 40 30 20 10 0

</div>
</div>
<div class="layoutArea">
<div class="column">
Mug Plate

</div>
</div>
</td>
<td>
<div class="layoutArea">
<div class="column">
Bottle Bowl Flask Fork Knife Mug Plate Spoon Teapot Vase

</div>
</div>
<div class="layoutArea">
<div class="column">
100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 20 0 0 0 0 0 0 0 0 0

0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0 34 0 0 0 0 0 0 0 0

0.0% 0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0 0 22 0 0 0 0 0 0 0

0.0% 0.0% 0.0% 72.7% 9.1% 0.0% 0.0% 18.2% 0.0% 0.0% 0008100200

0.0% 0.0% 0.0% 0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0 0 0 0 28 0 0 0 0 0

0.0% 2.0% 0.0% 0.0% 0.0% 98.0% 0.0% 0.0% 0.0% 0.0% 0 1 0 0 0 50 0 0 0 0

0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 100.0% 0.0% 0.0% 0.0% 0 0 0 0 0 0 23 0 0 0

0.0% 0.0% 0.0% 9.1% 3.0% 0.0% 0.0% 87.9% 0.0% 0.0% 0 0 0 3 1 0 0 29 0 0

0.0% 0.0% 0.0% 0.0% 0.0% 7.1% 0.0% 0.0% 92.9% 0.0% 0 0 0 0 0 2 0 0 26 0

0.0% 0.0% 0.0% 0.0% 0.0% 1.8% 0.0% 0.0% 0.0% 98.2% 0 0 0 0 0 1 0 0 0 56

</div>
</div>
<div class="layoutArea">
<div class="column">
Bottle

</div>
<div class="column">
Bowl Flask

</div>
<div class="column">
Fork Knife

Target Category

</div>
<div class="column">
Spoon

</div>
<div class="column">
Teapot Vase

</div>
</div>
<div class="layoutArea">
<div class="column">
Accuracy: 96.42%

</div>
<div class="column">
100 90 80 70 60 50 40 30 20 10 0

</div>
</div>
<div class="layoutArea">
<div class="column">
Mug Plate

</div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="layoutArea">
<div class="column">
Figure4: Confusionmatricesshowinghowwelleachmodelperformedinobjectrecognitiontaskonrestaurantobjectdataset.In each cell of a confusion matrix, we present the percentage and the absolute number of predictions. The darker diagonal cell shows the better prediction by the models.

Figure5: AsummaryofabunchofexperimentsfortheGOODdescriptorwithdiffidentKandvariousdistancefunctions:inthese experiments, we trained all data first. We then saved the perceptual memory to be used in other experiments.

References

[1] S.-H. Cha, ‚ÄúComprehensive survey on distance/similarity measures between probability density functions,‚Äù Interna- tional Journal of Mathematical Models and Methods in Applied Sciences, vol. 1, no. 4, pp. 300‚Äì307, 2007.

</div>
</div>
<div class="layoutArea">
<div class="column">
Bottle Bowl Flask Fork Knife Mug Plate Spoon Teapot Vase

</div>
</div>
<div class="layoutArea">
<div class="column">
100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 20 0 0 0 0 0 0 0 0 0

0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0 34 0 0 0 0 0 0 0 0

0.0% 0.0% 100.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0 0 22 0 0 0 0 0 0 0

0.0% 0.0% 0.0% 63.6% 18.2% 0.0% 0.0% 18.2% 0.0% 0.0% 0007200200

0.0% 0.0% 0.0% 0.0% 96.4% 0.0% 0.0% 3.6% 0.0% 0.0% 0 0 0 0 27 0 0 1 0 0

0.0% 2.0% 0.0% 0.0% 0.0% 98.0% 0.0% 0.0% 0.0% 0.0% 0 1 0 0 0 50 0 0 0 0

0.0% 4.3% 0.0% 0.0% 0.0% 0.0% 95.7% 0.0% 0.0% 0.0% 0 1 0 0 0 0 22 0 0 0

0.0% 0.0% 0.0% 3.0% 3.0% 0.0% 0.0% 93.9% 0.0% 0.0% 0 0 0 1 1 0 0 31 0 0

0.0% 0.0% 0.0% 0.0% 0.0% 7.1% 0.0% 0.0% 92.9% 0.0% 0 0 0 0 0 2 0 0 26 0

0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 0.0% 100.0% 0 0 0 0 0 0 0 0 0 57

</div>
</div>
<div class="layoutArea">
<div class="column">
Bottle

</div>
<div class="column">
Bowl Flask

</div>
<div class="column">
Fork Knife

Target Category

</div>
<div class="column">
Spoon

</div>
<div class="column">
Teapot Vase

</div>
</div>
<div class="layoutArea">
<div class="column">
Accuracy: 96.42%

</div>
<div class="column">
100 90 80 70 60 50 40 30 20 10 0

</div>
</div>
<div class="layoutArea">
<div class="column">
Mug Plate

</div>
</div>
<div class="layoutArea">
<div class="column">
5

</div>
</div>
</div>
<div class="section">
<div class="layoutArea">
<div class="column">
Predicted Category

</div>
</div>
<div class="layoutArea">
<div class="column">
Predicted Category

</div>
</div>
<div class="layoutArea">
<div class="column">
Predicted Category

</div>
</div>
</div>
</div>
<div class="page" title="Page 6">
<div class="section">
<div class="layoutArea">
<div class="column">
Part II: Test your approaches in a systematic open-ended scenario (50%)

The off-line evaluation methodologies are not well suited to evaluate open-ended learning systems, because they do not abide by the simultaneous nature of learning and recognition and also those methodologies imply that the set of categories must be predefined. We, therefore, adopted a teaching protocol designed for experimental evaluation in open-ended learning.

</div>
</div>
<div class="layoutArea">
<div class="column">
The idea is to emulate the interactions of a recogni- tion system with the surrounding environment over long periods of time in a single context scenario (of- fice, kitchen, etc.). The teacher follows a teaching protocol and interacts with the learning agent using three basic actions:

‚Ä¢ Teach: used for introducing a new object category to the agent;

‚Ä¢ Ask: used to ask the agent what is the category of a given object view;

‚Ä¢ Correct: used for providing corrective feedback in case of misclassification.

</div>
<div class="column">
Algorithm 1 Teaching protocol for performance evaluation 1: IntroduceCategory1

2: n‚Üê1

3: repeat

4: n ‚Üê n+1

5: Introduce Categoryn

6: k‚Üê0

7: c‚Üê1

8: repeat

9: Present a previously unseen instance of Categoryc

10: Ask the category of this instance

11: If needed, provide correct feedback

12: c‚Üê(c==n) ? 1 : c+1

13: k ‚Üê k+1

14: s ‚Üê success in last k question/correction iterations 15: until ((s &gt; œÑ and k &gt;= n)

16: or (user sees no improvement in success))

17: until(userseesnoimprovementinsuccess)

1

</div>
<div class="column">
‚óÉ Ready for the next category

‚óÉ question / correction iteration

‚óÉ accuracy threshold crossed ‚óÉ breakpoint reached ‚óÉbreakpointreached

</div>
</div>
<div class="layoutArea">
<div class="column">
Teaching protocol determines which examples are used for training the algorithm, and which are used to test the algorithm (see Algorithm 1). The protocol can be followed by a human teacher. However, replacing a human teacher with a simulated one makes it possible to conduct systematic, consistent and reproducible experiments for different approaches. It allows the possibility to perform multiple experiments and explore different experimental conditions in a fraction of time a human would take to carry out the same task. We, therefore, developed a simulated_teacher to follow the protocol and autonomously interact with the system. For this purpose, the simulated_teacher is connected to a large database of labeled object views. The complete process is summarized in Algorithm 1 and the overall system architecture is depicted in Fig. 6.

The idea is that the simulated_teacher repeatedly picks unseen object views from the currently known categories and presents them to the agent for testing. Inside the learning agent, the object view is recorded in the Perceptual Memory if it is marked as a training sample (i.e. whenever the teacher uses teach or correct instructions), otherwise it is sent to the Object Recognition module. The simulated_teacher continuously estimates the recognition performance of the agent using a sliding window of size 3n iterations, where n is the number of categories that have already been introduced. If k, the number of iterations since the last time a new category was introduced, is less than 3n, all results are used. In case this performance exceeds a given classification threshold (œÑ = 0.67, meaning accuracy is at least twice the error rate), the teacher introduces a new object category by presenting three randomly selected objects‚Äô views. In this way, the agent begins with zero knowledge and the training instances become gradually available according to the teaching protocol.

7 Breakpoint: In case the agent can not reach the classification threshold after a certain number of iterations (i.e. 100 iterations), the simulated teacher can infer that the agent is no longer able to learn more categories

</div>
</div>
<table>
<tbody>
<tr>
<td>
<div class="layoutArea">
<div class="column">
Simulated Teacher

</div>
</div>
<div class="layoutArea">
<div class="column">
Object Dataset

</div>
</div>
</td>
<td colspan="1" rowspan="2">
<div class="layoutArea">
<div class="column">
recognition result

</div>
</div>
</td>
<td>
<div class="layoutArea">
<div class="column">
Object Recognition

</div>
</div>
<div class="layoutArea">
<div class="column">
category models

</div>
<div class="column">
Perceptual Memory

</div>
</div>
<div class="layoutArea">
<div class="column">
deep object representation

</div>
</div>
<div class="layoutArea">
<div class="column">
previous object categories

object label

deep object representation

</div>
<div class="column">
new object categories

</div>
</div>
<div class="layoutArea">
<div class="column">
Object Representation

</div>
</div>
<div class="layoutArea">
<div class="column">
Object Conceptualization

</div>
</div>
</td>
<td colspan="1" rowspan="2"></td>
</tr>
<tr>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<div class="layoutArea">
<div class="column">
Figure 6: Interaction between the simulated teacher and the learning agent; (left) The simulated teacher is connected to a large object dataset and interacts with the agent by teach, ask and correct actions as shown by blue color; (right) In case of ask action, the agent is evaluated by a never-seen-before object. The agent recognizes the object and sends back the result to the simulated user; In the cases of teach and correct actions, the agent creates a new category model or updates the model of the respective category.

</div>
</div>
<div class="layoutArea">
<div class="column">
6

</div>
</div>
</div>
<div class="section">
<div class="layoutArea">
<div class="column">
Teach:

</div>
</div>
<div class="layoutArea">
<div class="column">
object views + label Ask: object view

</div>
</div>
<div class="layoutArea">
<div class="column">
Correct: label

</div>
</div>
</div>
</div>
<div class="page" title="Page 7">
<div class="layoutArea">
<div class="column">
and therefore, terminates the experiment. It is possible that the agent learns all existing categories before reach- ing the breaking point. In such a case, it is not possible to continue the protocol, and the experiment is halted. In your report, this should be shown by the stopping condition, ‚Äúlack of data‚Äù.

</div>
</div>
<div class="layoutArea">
<div class="column">
7 Dataset: In this experiment, one of the largest available 3D object datasets, namely Washington RGB-D Object Dataset is used. It consists of 250, 000 views of 300 objects and the objects are categorized into 51 categories. Figure 7 shows some example objects from the dataset. We have provided a short version of the dataset that each category only has 200 instances. We have already included the short version of the dataset (3.3 GB) into the virtual machine ($HOME/datasets/). Both versions are available online: ‚Äì short-version (3 GB) ‚Äì full-version (70 GB)

</div>
<div class="column">
Figure 7: Sample point clouds of objects in Wash- ington RGB-D Object Dataset.

</div>
</div>
<div class="layoutArea">
<div class="column">
√ê What we offer for this part

‚Ä¢ Provide the simulated teacher code to assess the performance of your code in open-ended settings.

‚Ä¢ Provide a set of MATLAB/Python codes to visualize the progress of the agent (related to task #3). $ python3 matlab_plots_parser.py -p PATH_TO_EXP_DIR/ ‚Äìonline

‚Ä¢ A bash script for running a bunch of experiments (find it out in rug_simulated_user/result folder). √ê Your tasks for this part

‚Ä¢ Based on the obtained results in the previous part (10-fold cross-validation experiments), select the best system configuration for both hand-crafted and deep transfer learning approaches (i.e., object representation + distance function). For each of the selected approaches, update the parameters of the simulated teacher in the launch files accordingly.

‚Ä¢ Since the order of introducing categories may have an effect on the performance of the system, you have to perform 10 experiments and report all 10 experiments plus the avg+std in a table (10 experiments for hand-crafted and 10 for deep-learning).

‚Ä¢ Visualize the following plots for the best learning progress of hand-crafted and deep transfer learning approaches (as an example, see Fig. 8), and compare them together (for further details on evaluation metrics and plots, please check out the OrthographciNet paper, which is available in the Nestor):

‚Äì protocol accuracy vs. #question/correction iterations (explain first 200 iterations) ‚Äì number of learned category vs. #question/correction iterations

‚Äì global classification accuracy vs. #question/correction iterations

‚Äì number of stored instances per category

 It should be noted that, instead of having diffident plots for each approach, you can visualize all your results together using the provided python script. Such visualization is really useful to analyse and compare the approaches. More information about the python parser is available on Nestor under

‚ÄúPractical assignments‚Äù tab.

‚Ä¢ The protocol_threshold parameter, œÑ, defines how good the agent should learn categories. For example, œÑ = 0.67 means the recognition accuracy is at least twice better than the error. There- fore, it can influence on all evaluation metrics. For each of the selected approaches, you need to per- form only three experiments by setting œÑ ‚àà [0.7,0.8,0.9], e.g., protocol_threshold:=0.7, and random_sequence_generator:=false to have fair comparison. Finally, you need to analyse the effect of œÑ based on the obtained results.

√ê How to run the experiments

Similar to the offline evaluation, we created a launch file for hand-crafted and deep transfer learning based algorithms.

However, before running an experiment, check the following items: 7

</div>
</div>
</div>
<div class="page" title="Page 8">
<div class="layoutArea">
<div class="column">
‚Ä¢ You have to update the value of different parameters of the system in the relative launch file.

 The system configuration is reported at the beginning of the report file of the experiment. Therefore,

you can use it as a way to debug/double-check the system‚Äôs parameters.

7 For hand-crafted based object representation approaches:

After setting a proper value for each of the system‚Äôs parameter, you can run an open-ended object recognition experiment

using the following command:

<pre>$ roslaunch rug_simulated_user simulated_user_hand_crafted_descriptor.launch
</pre>
7 For deep learning based object representation approaches:

Similar to the offline evaluation for deep learning based approaches, you need to open three terminals and use the

following commands to run an open-ended object recognition experiment for an specific network architecture:

aÃÄ MobileNetV2 Architecture

$ roscore

$ rosrun rug_deep_feature_extraction multi_view_RGBD_object_representation.py mobileNetV2 $ roslaunch rug_simulated_user simulated_user_RGBD_deep_learning_descriptor.launch ortho

<pre> graphic_image_resolution:=150 base_network:=mobileNetV2 K_for_KNN:=7 name_of_approach:=TEST
</pre>
aÃÄ VGG16 Architecture

$ roscore

$ rosrun rug_deep_feature_extraction multi_view_object_RGBD_representation.py vgg16_fc1 $ roslaunch rug_simulated_user simulated_user_RGBD_deep_learning_descriptor.launch ortho

<pre> graphic_image_resolution:=150 base_network:=vgg16_fc1 K_for_KNN:=7 name_of_approach:=TEST
</pre>
To have a fair comparison, the order of introducing categories should be same in both approaches. Therefore, we design a Boolean parameter named random_sequence_generator that can be used for this purpose. Check out the script we have provided for more details.

√ê What are the outputs of each experiment

<ul>
<li>Results of an experiment, including a detail summary and a set of MATLAB files (see Fig. 8), will be saved in:
<pre>      $HOME/student_ws/rug_simulated_user/result/experiment_1/
</pre>
After each experiment, you need to either rename the experiment_1 folder or move it to another folder, otherwise its contents will be replaced by the results of a new experiment.
</li>
<li>The system also reports a summary of a bunch of experiments as a txt file in the following path : rug_simulated_user/result/results_of_name_of_approach_experiments.txt
 Each time you run an experiment, the experiment results will be automatically appended to the log file. After running a bunch of 10 experiments, you have to report the content of the log file as a table in your report, compare the obtained results, and visualize the output of the best experiment for hand-crafted and deep transfer learning experiments (as an example see Fig. 8).

√ê Extra credit

To be eligible for the extra credit points, your approach (object representation or recognition) should be different than the provided sample codes. We will evaluate your object recognition approach using the same simulated teacher code. We will add 0.5 to the final score of the student who achieves the highest performance, 0.35 points to the student who achieves the second place, and 0.20 points to the student who achieves third place. We will compute the performance of your algorithm ourselves (code that does not run will be disqualified from the contest). This reward is designed to encourage you to experiment with different algorithms and hyperparameter settings to obtain the best performance.
</li>
</ul>
</div>
</div>
<div class="layoutArea">
<div class="column">
8

</div>
</div>
</div>
<div class="page" title="Page 9">
<div class="section">
<table>
<tbody>
<tr>
<td>
<div class="layoutArea">
<div class="column">
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0

</div>
</div>
<div class="layoutArea">
<div class="column">
0 20 40 60 80 100 120 140 160 180 200 Question / Correction Iterations

</div>
</div>
</td>
</tr>
<tr>
<td>
<div class="layoutArea">
<div class="column">
1.05 1

0.95 0.9 0.85 0.8 0.75 0.7

</div>
</div>
<div class="layoutArea">
<div class="column">
0 5 10 15 20 25 30 35 40 45 50 55 Number of Learned Categories

</div>
</div>
</td>
</tr>
<tr>
<td>
<div class="layoutArea">
<div class="column">
60

50

40

30

20

10

0

</div>
</div>
<div class="layoutArea">
<div class="column">
0 200

</div>
<div class="column">
400 600 800 Question / Correction Iterations

</div>
<div class="column">
1000 1200 1400

</div>
</div>
</td>
</tr>
<tr>
<td>
<div class="layoutArea">
<div class="column">
25

20

15

10

5

0

</div>
</div>
</td>
</tr>
</tbody>
</table>
<div class="layoutArea">
<div class="column">
Figure 8: Summary of an example open-ended experiment: (top) Evolution of protocol accuracy over first 200 iterations. In this experiment, the protocol threshold is set to 0.67, as shown by the green horizontal dashed line. Once the agent has classified all learned categories at least once, and the protocol accuracy is higher than the threshold, a new category is introduced, which is shown by a red dashed line and a category name. (second) This plot shows global classification accuracy as a function of number of learned categories for the same experiment. It can be seen that the agent was able to stay above the protocol threshold during the entire experiment, indicating the agent can learn many more categories. (third) This graph represents how fast does the agent learn by representing the number of learned categories as a function of the number of question/correction iterations. (bottom) This graph shows the number of instances stored in each category, i.e., the three instances provided at the introduction of the category together with the instances that had to be corrected somewhere along the experiment run. The ball category apparently was the most difficult one, requiring the largest number of instances.

Submission

A report (i.e., up to four pages ‚Äì two pages for the first part and two pages for the second part ‚Äì IEEE conference format, containing all figures, tables, and references) has to be delivered. You need to include an ‚ÄúAuthors‚Äô Contributions‚Äù section at the end of the report explaining how did you divide the work among the members, and what are the contributions of each author. We expect all authors contribute equally to the assignment. Submit your assignment in as a pdf file named group_number_prj1.pdf

U Do not delete your results after submitting the report. We may ask you to send us your rug_kfold_cross_validation and rug_simulated_user packages and the obtained results.

</div>
</div>
<div class="layoutArea">
<div class="column">
9

</div>
</div>
</div>
<div class="section">
<div class="layoutArea">
<div class="column">
Number of Stored Instances Number of Learned Categories Global Classification Accuracy Protocol Accuracy

</div>
</div>
</div>
<div class="section">
<div class="layoutArea">
<div class="column">
instant-noodles cup-food

</div>
</div>
<div class="layoutArea">
<div class="column">
jar-food shampoo

</div>
</div>
<div class="layoutArea">
<div class="column">
hand-towel box-food

</div>
</div>
<div class="layoutArea">
<div class="column">
glue-stick

comb

</div>
</div>
<div class="layoutArea">
<div class="column">
bag-food pitcher

</div>
</div>
<div class="layoutArea">
<div class="column">
kleenex mushroom

</div>
</div>
<div class="layoutArea">
<div class="column">
bell-pepper calculator

</div>
</div>
<div class="layoutArea">
<div class="column">
tomato toothpaste

</div>
</div>
<div class="layoutArea">
<div class="column">
lemon

bowl

</div>
</div>
<div class="layoutArea">
<div class="column">
peach water-bottle

</div>
</div>
<div class="layoutArea">
<div class="column">
banana sponge

</div>
</div>
<div class="layoutArea">
<div class="column">
marker binder

</div>
</div>
<div class="layoutArea">
<div class="column">
cell-phone keyboard

</div>
</div>
<div class="layoutArea">
<div class="column">
ball scissors

</div>
</div>
<div class="layoutArea">
<div class="column">
camera apple

</div>
</div>
<div class="layoutArea">
<div class="column">
can-food stapler

</div>
</div>
<div class="layoutArea">
<div class="column">
toothbrush soda-can

</div>
</div>
<div class="layoutArea">
<div class="column">
notebook potato

</div>
</div>
<div class="layoutArea">
<div class="column">
onion coffee-mug

</div>
</div>
<div class="layoutArea">
<div class="column">
plate pliers

</div>
</div>
<div class="layoutArea">
<div class="column">
pear

cap

</div>
</div>
<div class="layoutArea">
<div class="column">
lightbulb orange

</div>
</div>
<div class="layoutArea">
<div class="column">
greens flashlight

</div>
</div>
<div class="layoutArea">
<div class="column">
dry-battery rubber-eraser

</div>
</div>
<div class="layoutArea">
<div class="column">
cereal-box

lime

</div>
</div>
<div class="layoutArea">
<div class="column">
garlic

</div>
</div>
</div>
</div>
